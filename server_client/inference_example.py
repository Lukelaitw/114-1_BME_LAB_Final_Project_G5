"""
CTNetæ¨ç†ä½¿ç”¨ç¤ºä¾‹

é€™å€‹æ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨inference.pyé€²è¡Œæ¨ç†çš„å¹¾ç¨®æ–¹å¼

é‡è¦æç¤ºï¼š
- å°æ–¼LOSOäº¤å‰é©—è­‰ï¼Œæ‡‰è©²ä½¿ç”¨ CTNetEnsembleInference ä¾†åŠ è¼‰æ‰€æœ‰æ¨¡å‹ä¸¦é€²è¡Œå¹³å‡é æ¸¬
- å–®å€‹æ¨¡å‹æ¨ç†ï¼ˆCTNetInferenceï¼‰åªé©ç”¨æ–¼å–®å€‹å—è©¦è€…çš„æ¨¡å‹
- Ensembleæ–¹æ³•é€šå¸¸èƒ½æä¾›æ›´ç©©å®šå’Œæº–ç¢ºçš„é æ¸¬çµæœ
"""

import numpy as np
from inference import CTNetInference, CTNetEnsembleInference


# ========== æ–¹å¼4: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆä½¿ç”¨Ensembleï¼‰ ==========
def example_realtime_classification():
    """å¯¦æ™‚åˆ†é¡åˆ†æç¤ºä¾‹ - ä½¿ç”¨æ»‘å‹•çª—å£è™•ç†é€£çºŒæ•¸æ“šæµ"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹4: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆæ»‘å‹•çª—å£ï¼‰")
    print("=" * 50)
    
    # ä½¿ç”¨Ensembleæ¨ç†å™¨
    model_dir = "Loso_C_heads_2_depth_8_0"
    inferencer = CTNetEnsembleInference(
        model_dir=model_dir,
        dataset_type='C',
        heads=2, emb_size=16, depth=8,
        eeg1_f1=8, eeg1_kernel_size=64, eeg1_D=2,
        eeg1_pooling_size1=8, eeg1_pooling_size2=8,
        eeg1_dropout_rate=0.25, flatten_eeg1=240
    )
    
    # æ–¹å¼1: å¾æ–‡ä»¶å¯¦æ™‚è®€å–ï¼ˆæ¨¡æ“¬å¯¦æ™‚æ•¸æ“šæµï¼‰
    print("\n--- æ–¹å¼1: å¾æ–‡ä»¶å¯¦æ™‚è®€å– ---")
    txt_file = "bci_dataset_113-2/S01/2.txt"
    
    try:
        # è®€å–å®Œæ•´æ•¸æ“š
        full_data = np.loadtxt(txt_file, dtype=np.float32)
        print(f"æ•¸æ“šæ–‡ä»¶: {txt_file}")
        print(f"æ•¸æ“šé•·åº¦: {len(full_data)} å€‹æ¨£æœ¬")
        
        # å®šç¾©æ•¸æ“šæµç”Ÿæˆå™¨ï¼ˆæ¨¡æ“¬å¯¦æ™‚æ¥æ”¶æ•¸æ“šï¼‰
        def data_stream_generator(data, chunk_size=200):
            """æ¨¡æ“¬å¯¦æ™‚æ•¸æ“šæµï¼Œæ¯æ¬¡è¿”å›ä¸€å°å¡Šæ•¸æ“š"""
            for i in range(0, len(data), chunk_size):
                yield data[i:i+chunk_size]
        
        # å®šç¾©å›èª¿å‡½æ•¸ï¼ˆæ¯æ¬¡é æ¸¬å¾Œèª¿ç”¨ï¼‰
        class_names = ['æ”¾é¬†', 'å°ˆæ³¨']  # æ ¹æ“šä½ çš„æ•¸æ“šé›†èª¿æ•´
        
        def prediction_callback(result):
            """å¯¦æ™‚é¡¯ç¤ºé æ¸¬çµæœ"""
            pred = result['prediction']
            prob = result['probability']
            window_idx = result['window_idx']
            
            # æ ¼å¼åŒ–è¼¸å‡º
            status = "ğŸŸ¢" if pred == 1 else "ğŸ”µ"
            print(f"çª—å£ {window_idx:4d} | {status} é æ¸¬: {class_names[pred]:4s} | "
                  f"æ¦‚ç‡: [{prob[0]:.3f}, {prob[1]:.3f}] | "
                  f"ç½®ä¿¡åº¦: {max(prob)*100:.1f}%")
        
        # é€²è¡Œå¯¦æ™‚æ¨ç†
        print("\né–‹å§‹å¯¦æ™‚åˆ†æ...")
        print("-" * 70)
        
        results_list = []
        for result in inferencer.predict_realtime(
            data_stream_generator(full_data, chunk_size=200),
            window_size=1000,      # çª—å£å¤§å°
            stride=100,            # æ»‘å‹•æ­¥é•·ï¼ˆè¶Šå°è¶Šå¯¦æ™‚ï¼‰
            smoothing_window=5,     # å¹³æ»‘çª—å£ï¼ˆæ¸›å°‘è·³å‹•ï¼‰
            callback=prediction_callback
        ):
            results_list.append(result)
        
        print("-" * 70)
        print(f"\nåˆ†æå®Œæˆï¼å…±è™•ç† {len(results_list)} å€‹çª—å£")
        
        # çµ±è¨ˆçµæœ
        predictions = [r['prediction'] for r in results_list]
        from collections import Counter
        pred_counts = Counter(predictions)
        print(f"\né æ¸¬çµ±è¨ˆ:")
        for cls_idx, count in pred_counts.items():
            percentage = count / len(predictions) * 100
            print(f"  {class_names[cls_idx]}: {count} æ¬¡ ({percentage:.1f}%)")
        
        return results_list
        
    except FileNotFoundError:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {txt_file}")
        print("è«‹ç¢ºä¿æ•¸æ“šæ–‡ä»¶å­˜åœ¨")
        return None


# ========== æ–¹å¼4b: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆå¾æ•¸çµ„ï¼‰ ==========
def example_realtime_from_array():
    """å¾æ•¸çµ„é€²è¡Œå¯¦æ™‚åˆ†é¡åˆ†æ"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹4b: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆå¾æ•¸çµ„ï¼‰")
    print("=" * 50)
    
    # ä½¿ç”¨Ensembleæ¨ç†å™¨
    model_dir = "Loso_C_heads_2_depth_8_0"
    inferencer = CTNetEnsembleInference(
        model_dir=model_dir,
        dataset_type='C',
        heads=2, emb_size=16, depth=8,
        eeg1_f1=8, eeg1_kernel_size=64, eeg1_D=2,
        eeg1_pooling_size1=8, eeg1_pooling_size2=8,
        eeg1_dropout_rate=0.25, flatten_eeg1=240
    )
    
    # å‰µå»ºæ¨¡æ“¬æ•¸æ“šï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æ›¿æ›ç‚ºçœŸå¯¦æ•¸æ“šæµï¼‰
    print("ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šé€²è¡Œå¯¦æ™‚åˆ†æ...")
    simulated_data = np.random.randn(5000).astype(np.float32)  # 5000å€‹æ¨£æœ¬
    
    class_names = ['æ”¾é¬†', 'å°ˆæ³¨']
    results_list = []
    
    print("\nå¯¦æ™‚é æ¸¬çµæœ:")
    print("-" * 70)
    
    for result in inferencer.predict_realtime(
        simulated_data,
        window_size=1000,
        stride=200,           # æ¯200å€‹æ¨£æœ¬æ»‘å‹•ä¸€æ¬¡
        smoothing_window=3,   # ä½¿ç”¨3å€‹çª—å£çš„ç§»å‹•å¹³å‡
        callback=None          # ä¸ä½¿ç”¨å›èª¿ï¼Œç›´æ¥è™•ç†çµæœ
    ):
        pred = result['prediction']
        prob = result['probability']
        window_idx = result['window_idx']
        
        status = "ğŸŸ¢" if pred == 1 else "ğŸ”µ"
        print(f"çª—å£ {window_idx:3d} | {status} {class_names[pred]:4s} | "
              f"æ¦‚ç‡: [{prob[0]:.3f}, {prob[1]:.3f}]")
        
        results_list.append(result)
        
        # åªé¡¯ç¤ºå‰20å€‹çª—å£ï¼Œé¿å…è¼¸å‡ºéå¤š
        if window_idx >= 19:
            print("... (çœç•¥å¾ŒçºŒçµæœ)")
            break
    
    print("-" * 70)
    print(f"\nè™•ç†å®Œæˆï¼å…± {len(results_list)} å€‹çª—å£")
    
    return results_list


# ========== æ–¹å¼5: ä½¿ç”¨çœŸå¯¦æ•¸æ“šé€²è¡Œæ¨ç†ï¼ˆä½¿ç”¨Ensembleï¼‰ ==========
def example_real_data_inference():
    """ä½¿ç”¨çœŸå¯¦æ¸¬è©¦æ•¸æ“šé€²è¡Œæ¨ç† - ä½¿ç”¨Ensemble"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹4: ä½¿ç”¨çœŸå¯¦æ¸¬è©¦æ•¸æ“šæ¨ç†ï¼ˆEnsembleï¼‰")
    print("=" * 50)
    
    from utils import load_data_evaluate
    
    # ä½¿ç”¨Ensembleæ¨ç†å™¨
    model_dir = "Loso_C_heads_2_depth_8_0"
    inferencer = CTNetEnsembleInference(
        model_dir=model_dir,
        dataset_type='C',
        heads=2, emb_size=16, depth=8,
        eeg1_f1=8, eeg1_kernel_size=64, eeg1_D=2,
        eeg1_pooling_size1=8, eeg1_pooling_size2=8,
        eeg1_dropout_rate=0.25, flatten_eeg1=240
    )
    
    # åŠ è¼‰æ¸¬è©¦æ•¸æ“š
    data_dir = "./bci_dataset_113-2/"
    train_data, train_label, test_data, test_label = load_data_evaluate(
        data_dir, 'C', 1, mode_evaluate='LOSO'
    )
    
    # æº–å‚™æ¸¬è©¦æ•¸æ“šï¼ˆéœ€è¦æ·»åŠ channelç¶­åº¦ï¼‰
    test_data = np.expand_dims(test_data, axis=1)  # (n_trials, 1, 1000)
    
    # è¨ˆç®—æ¨™æº–åŒ–åƒæ•¸ï¼ˆä½¿ç”¨è¨“ç·´æ•¸æ“šï¼‰
    train_mean = np.mean(train_data)
    train_std = np.std(train_data)
    inferencer.set_normalization_params(train_mean, train_std)
    
    # é€²è¡Œé æ¸¬
    predictions, probs = inferencer.predict(test_data[:10], return_probs=True)  # åªæ¸¬è©¦å‰10å€‹æ¨£æœ¬
    
    # è™•ç†æ¨™ç±¤å½¢ç‹€ï¼ˆç¢ºä¿æ˜¯1Dé™£åˆ—ï¼‰
    true_labels = test_label[:10]
    if true_labels.ndim > 1:
        true_labels = true_labels.flatten()  # å°‡ (n, 1) è½‰æ›ç‚º (n,)
    true_labels = true_labels - 1  # è½‰æ›ç‚º0-indexed
    
    print(f"æ¸¬è©¦æ•¸æ“šå‰10å€‹æ¨£æœ¬:")
    print(f"çœŸå¯¦æ¨™ç±¤: {true_labels}")
    print(f"é æ¸¬æ¨™ç±¤: {predictions}")
    
    # è¨ˆç®—æº–ç¢ºç‡
    correct = (predictions == true_labels).sum()
    accuracy = correct / len(predictions)
    print(f"æº–ç¢ºç‡: {accuracy:.4f} ({correct}/{len(predictions)})")
    
    # é¡¯ç¤ºæ¯å€‹æ¨£æœ¬çš„è©³ç´°ä¿¡æ¯
    print("\nè©³ç´°çµæœ:")
    for i in range(len(predictions)):
        print(f"  æ¨£æœ¬ {i+1}: çœŸå¯¦={true_labels[i]}, é æ¸¬={predictions[i]}, "
              f"æ¦‚ç‡=[é¡åˆ¥0={probs[i][0]:.4f}, é¡åˆ¥1={probs[i][1]:.4f}]")


if __name__ == "__main__":
    # é‹è¡Œç¤ºä¾‹
    print("CTNetæ¨ç†ç¤ºä¾‹\n")
    
    # ç¤ºä¾‹1: å–®å€‹æ¨£æœ¬
   # example_single_sample()
    
    # ç¤ºä¾‹2: æ‰¹é‡æ¨ç†
   # example_batch_inference()
    
    # ç¤ºä¾‹3: å¾æ–‡ä»¶æ¨ç†
    # example_file_inference()
    
    # ç¤ºä¾‹3b: æ¨ç†ä¸¦åŒæ™‚è©•ä¼°ï¼ˆæ¨è–¦ï¼‰
    # example_inference_with_evaluation()
    
    # ç¤ºä¾‹4: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆæ¨è–¦ç”¨æ–¼å¯¦æ™‚æ‡‰ç”¨ï¼‰
    example_realtime_classification()
    
    # ç¤ºä¾‹4b: å¯¦æ™‚åˆ†é¡åˆ†æï¼ˆå¾æ•¸çµ„ï¼‰
    # example_realtime_from_array()
    
    # ç¤ºä¾‹5: çœŸå¯¦æ•¸æ“šæ¨ç†
   # example_real_data_inference()
    
    print("\n" + "=" * 50)
    print("æ‰€æœ‰ç¤ºä¾‹é‹è¡Œå®Œæˆï¼")
    print("=" * 50)
